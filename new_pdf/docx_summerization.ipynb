{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8d671b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from docx import Document\n",
    "\n",
    "# def split_docx_pages(input_docx_path, output_dir):\n",
    "#     # Open the Word document\n",
    "#     doc = Document(input_docx_path)\n",
    "\n",
    "#     # Iterate over each section in the document\n",
    "#     for i, section in enumerate(doc.sections):\n",
    "#         # Create a new Document object for this page\n",
    "#         new_doc = Document()\n",
    "\n",
    "#         # Copy the content from the original document to the new document\n",
    "#         for element in section.body:\n",
    "#             new_doc._body.append(element)\n",
    "\n",
    "#         # Save the new document to a file\n",
    "#         new_doc_path = f\"{output_dir}/page_{i + 1}.docx\"\n",
    "#         new_doc.save(new_doc_path)\n",
    "\n",
    "#         print(f\"Page {i + 1} saved as {new_doc_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# input_docx_path = \"New User Adoption Execution Steps.docx\"\n",
    "# output_dir = \"output_pages\"\n",
    "# split_docx_pages(input_docx_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b222bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aspose-words\n",
      "  Obtaining dependency information for aspose-words from https://files.pythonhosted.org/packages/6e/7f/1beb1efa0766f6fc83b51ea7f3cd611d5b08c04a1cd6d457f013f7c8b20e/aspose_words-24.3.0-py3-none-win_amd64.whl.metadata\n",
      "  Downloading aspose_words-24.3.0-py3-none-win_amd64.whl.metadata (8.5 kB)\n",
      "Downloading aspose_words-24.3.0-py3-none-win_amd64.whl (62.8 MB)\n",
      "   ---------------------------------------- 0.0/62.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/62.8 MB 1.8 MB/s eta 0:00:36\n",
      "   ---------------------------------------- 0.3/62.8 MB 2.5 MB/s eta 0:00:26\n",
      "   ---------------------------------------- 0.7/62.8 MB 3.8 MB/s eta 0:00:17\n",
      "    --------------------------------------- 1.0/62.8 MB 4.4 MB/s eta 0:00:14\n",
      "    --------------------------------------- 1.3/62.8 MB 4.7 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 1.6/62.8 MB 5.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.9/62.8 MB 5.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 2.3/62.8 MB 5.7 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 2.8/62.8 MB 6.1 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 3.2/62.8 MB 6.3 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 3.6/62.8 MB 6.3 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 3.9/62.8 MB 6.6 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 4.3/62.8 MB 6.6 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 4.7/62.8 MB 6.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 5.2/62.8 MB 6.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 5.6/62.8 MB 7.0 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 5.9/62.8 MB 7.0 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 6.2/62.8 MB 6.9 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 6.5/62.8 MB 7.0 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 6.7/62.8 MB 6.8 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 7.1/62.8 MB 6.9 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 7.4/62.8 MB 6.8 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 7.7/62.8 MB 6.8 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 8.0/62.8 MB 6.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 8.4/62.8 MB 6.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 8.6/62.8 MB 6.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 9.1/62.8 MB 6.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 9.3/62.8 MB 6.9 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 9.7/62.8 MB 6.9 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 10.1/62.8 MB 6.9 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 10.5/62.8 MB 7.4 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 11.0/62.8 MB 7.4 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 11.5/62.8 MB 7.7 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 12.0/62.8 MB 7.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 12.1/62.8 MB 7.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 12.5/62.8 MB 7.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 13.0/62.8 MB 7.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 13.6/62.8 MB 7.8 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 14.0/62.8 MB 7.8 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 14.4/62.8 MB 7.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 14.9/62.8 MB 7.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 15.3/62.8 MB 7.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 15.8/62.8 MB 8.0 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 16.3/62.8 MB 8.1 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 16.8/62.8 MB 8.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 17.3/62.8 MB 8.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 17.6/62.8 MB 8.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 17.9/62.8 MB 8.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 18.4/62.8 MB 8.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 18.9/62.8 MB 9.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 19.3/62.8 MB 9.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 19.7/62.8 MB 9.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 20.3/62.8 MB 9.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 20.8/62.8 MB 9.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 21.4/62.8 MB 9.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 22.0/62.8 MB 9.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 22.3/62.8 MB 9.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 22.6/62.8 MB 9.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 23.1/62.8 MB 9.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 23.6/62.8 MB 9.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 24.1/62.8 MB 9.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 24.5/62.8 MB 9.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 25.0/62.8 MB 9.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 25.2/62.8 MB 9.4 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 25.6/62.8 MB 9.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 25.9/62.8 MB 9.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 26.3/62.8 MB 9.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 26.8/62.8 MB 9.1 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 27.4/62.8 MB 9.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 27.8/62.8 MB 9.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 28.3/62.8 MB 9.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 28.7/62.8 MB 9.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 29.1/62.8 MB 9.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 29.6/62.8 MB 9.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 30.0/62.8 MB 9.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 30.4/62.8 MB 9.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 30.8/62.8 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 31.4/62.8 MB 9.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 31.7/62.8 MB 9.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 32.3/62.8 MB 9.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 32.6/62.8 MB 8.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 33.0/62.8 MB 9.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 33.5/62.8 MB 9.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 34.0/62.8 MB 9.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 34.4/62.8 MB 9.1 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 34.5/62.8 MB 8.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 34.9/62.8 MB 9.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 35.3/62.8 MB 8.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 35.5/62.8 MB 8.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 35.9/62.8 MB 8.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 36.5/62.8 MB 8.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 36.7/62.8 MB 8.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 37.4/62.8 MB 8.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 38.0/62.8 MB 9.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 38.4/62.8 MB 8.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 38.7/62.8 MB 9.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 39.2/62.8 MB 8.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 39.6/62.8 MB 8.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 40.2/62.8 MB 8.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 40.6/62.8 MB 8.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 40.9/62.8 MB 8.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 41.4/62.8 MB 8.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 41.7/62.8 MB 8.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 42.1/62.8 MB 8.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 42.5/62.8 MB 8.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 42.9/62.8 MB 8.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 43.3/62.8 MB 8.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 43.8/62.8 MB 8.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 44.2/62.8 MB 8.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 44.5/62.8 MB 8.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 45.1/62.8 MB 8.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 45.5/62.8 MB 9.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 46.1/62.8 MB 9.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 46.7/62.8 MB 9.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 47.0/62.8 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 47.6/62.8 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 48.1/62.8 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 48.4/62.8 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 48.8/62.8 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 49.4/62.8 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 49.9/62.8 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 50.4/62.8 MB 9.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 51.0/62.8 MB 9.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 51.6/62.8 MB 9.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 52.1/62.8 MB 10.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 52.6/62.8 MB 10.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 53.1/62.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 53.6/62.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 54.2/62.8 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 54.7/62.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 55.2/62.8 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 55.7/62.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 56.0/62.8 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 56.2/62.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 56.8/62.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 57.4/62.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 58.0/62.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 58.5/62.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 59.0/62.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 59.5/62.8 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 60.1/62.8 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 60.7/62.8 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 61.1/62.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  61.7/62.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  62.3/62.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  62.8/62.8 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  62.8/62.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  62.8/62.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 62.8/62.8 MB 8.7 MB/s eta 0:00:00\n",
      "Installing collected packages: aspose-words\n",
      "Successfully installed aspose-words-24.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install aspose-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db1ef3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<aspose.words.saving.SaveOutputParameters object at 0x000001E403756510>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aspose.words as aw\n",
    "\n",
    "# load Word document\n",
    "doc = aw.Document(\"New User Adoption Execution Steps.docx\")\n",
    "\n",
    "# extract range of pages\n",
    "extractedPages = doc.extract_pages(3, 6)\n",
    "\n",
    "# save pages as a separate document\n",
    "extractedPages.save(\"split_by_page_range.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f1504a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aspose.words as aw\n",
    "\n",
    "# load Word document\n",
    "doc = aw.Document(\"New User Adoption Execution Steps.docx\")\n",
    "\n",
    "# get page count\n",
    "pageCount = doc.page_count\n",
    "\n",
    "# loop through pages\n",
    "for page in range(0, pageCount):\n",
    "  \n",
    "    # save each page as a separate document\n",
    "    extractedPage = doc.extract_pages(page, 1)\n",
    "    extractedPage.save(f\"split_by_page_{page + 1}.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a25ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_titles=[\"split_by_page_1\",\"split_by_page_2\",\"split_by_page_3\",\"split_by_page_4\",\"split_by_page_5\",\"split_by_page_6\",\"split_by_page_7\",\"split_by_page_8\",\"split_by_page_9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b04ffe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, get_response_synthesizer\n",
    "from llama_index.core import DocumentSummaryIndex\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fdb05110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load documents\n",
    "# city_docs = []\n",
    "# for wiki_title in wiki_titles:\n",
    "#     docs = SimpleDirectoryReader(\n",
    "#         input_files=[f\"./{wiki_title}.docx\"]  # Change the file extension to .doc or any other desired extension\n",
    "#     ).load_data()\n",
    "#     docs[0].doc_id = wiki_title\n",
    "#     city_docs.extend(docs)\n",
    "# import os\n",
    "# import PyPDF2\n",
    "\n",
    "# def load_pdf_files(pdf_files):\n",
    "#     loaded_docs = []\n",
    "#     for pdf_file in pdf_files:\n",
    "#         with open(pdf_file, 'rb') as file:\n",
    "#             pdf_reader = PyPDF2.PdfReader(file)\n",
    "#             pdf_text = \"\"\n",
    "#             for page in pdf_reader.pages:\n",
    "#                 pdf_text += page.extract_text()\n",
    "#             loaded_docs.append(pdf_text)\n",
    "#     return loaded_docs\n",
    "\n",
    "# # Example usage\n",
    "# pdf_files = ['./file1.pdf', './file2.pdf']  # List of PDF file paths\n",
    "# city_docs = load_pdf_files(pdf_files)\n",
    "city_docs = []\n",
    "for wiki_title in wiki_titles:\n",
    "    docs = SimpleDirectoryReader(\n",
    "        input_files=[f\"./{wiki_title}.docx\"]\n",
    "    ).load_data()\n",
    "    docs[0].doc_id = wiki_title\n",
    "    city_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8d05c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceSplitter(chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87acef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9de6b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0bb19a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_together import Together\n",
    "# splitter = SentenceSplitter(chunk_size=1000)\n",
    "# llm= Together(\n",
    "#         model=\"togethercomputer/llama-2-70b-chat\",\n",
    "#         # model=\"gpt-4\",\n",
    "#         temperature=0.1,\n",
    "# #         max_tokens=207,\n",
    "#         top_k=1,\n",
    "#         together_api_key=\"1cecf43792b1187b044ab0293853353cc45caf8fdfe0a82e049d53cbf5954d26\",\n",
    "#     )\n",
    "from llama_index.llms.together import TogetherLLM\n",
    "llm = TogetherLLM(\n",
    "    model=\"togethercomputer/llama-2-70b-chat\", api_key=\"96557b956acf6073510ee7e4abadc1c7863626e75278a0eaf9a747875af30604\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "15e39844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.together import TogetherEmbedding\n",
    "\n",
    "# embed_model = TogetherEmbedding(\n",
    "#     model_name=\"togethercomputer/m2-bert-80M-8k-retrieval\", api_key=\"1cecf43792b1187b044ab0293853353cc45caf8fdfe0a82e049d53cbf5954d26\")\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "lc_embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "embed_model = LangchainEmbedding(lc_embed_model)\n",
    "Settings.lc_embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6c3ab700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "Settings.context_window = 3000\n",
    "Settings.llm =llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5fbb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm =llm\n",
    "# from llama_index.core import Settings\n",
    "# Settings.context_window = 4096\n",
    "# Settings.llm =llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40422a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a5501815e54182881e9fae7601a263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609be9a5373f4f0d92b1382f7a29402d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing documents:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: split_by_page_1\n",
      "current doc id: split_by_page_2\n",
      "current doc id: split_by_page_3\n",
      "current doc id: split_by_page_4\n",
      "current doc id: split_by_page_5\n",
      "current doc id: split_by_page_6\n",
      "current doc id: split_by_page_7\n",
      "current doc id: split_by_page_8\n",
      "current doc id: split_by_page_9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f369f680f7e499c8cc59b177d82baf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    city_docs,\n",
    "    llm=llm,\n",
    "    transformations=[splitter],\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8e45778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  The provided text appears to be a set of instructions or steps for refreshing a dashboard related to user adoption. The text mentions taking backups of various dashboards and databases, and then downloading data in Excel format. It also mentions adding columns to the Excel sheet.\\n\\nSome questions that this text can answer include:\\n\\n* What are the steps involved in refreshing a user adoption dashboard?\\n* How do you take backups of dashboards and databases?\\n* How do you download data from a dashboard into Excel format?\\n* How do you add columns to an Excel sheet?\\n* What is the purpose of refreshing a user adoption dashboard?\\n* What kind of data is used to refresh a user adoption dashboard?', '  The provided text appears to be a documentation or a report that discusses various business-related topics, including country names, column names, functions, and dashboards. It also mentions a software tool called Aspose.Words, which is used to create and manipulate Word documents.\\n\\nThe text can answer questions such as:\\n\\n* What are the different countries mentioned in the text?\\n* What are the various functions and dashboards discussed in the text?\\n* What is the purpose of the \"Name hist view\" column?\\n* What is the difference between a cockpit and a non-cockpit dashboard?\\n* What is the software tool Aspose.Words used for?\\n* What is the purpose of the QARA Dashboard?\\n\\nOverall, the text seems to be providing information related to business operations, data analysis, and software tools used for these purposes.', '  The provided text appears to be a set of instructions or a guide for working with data in a business context. It mentions various dashboards, data sources, and regions, and provides a query for selecting data based on the local date and time.\\n\\nSome of the questions that this text can answer include:\\n\\n* How do I filter data based on the local date and time in SQL?\\n* What is the purpose of the Aspose.Words software, and what are its capabilities?\\n* How do I download sales and RA dashboards for different regions?\\n* What is the format of the data in the APAC_USAGE_HIST_SNAPSHOT_ALL_QTR table, and what information does it contain?\\n* How do I use the convert() function in SQL to convert a date and time field to a datetime data type?\\n* What is the difference between the All Business and RA dashboards, and how do they relate to the data in the APAC_USAGE_HIST_SNAPSHOT_ALL_QTR table?', '  The provided text appears to be a set of instructions or procedures for carrying out a specific task. The text mentions various software tools and applications, including Aspose.Words, Excel, SSMS, and a remote desktop connection. The task involves preparing excel sheets for different regions, copying and pasting data into a remote desktop, and importing data into a database.\\n\\nSome of the questions that this text can answer include:\\n\\n* What are the steps involved in preparing excel sheets for different regions?\\n* How do you copy and paste data into a remote desktop using SSMS?\\n* What is the procedure for importing data into a database using SSMS?\\n* What software tools are required to carry out the task described in the text?\\n* What is the purpose of the task described in the text?\\n\\nIt is important to note that the text does not provide a clear context or purpose for the task, and some of the steps mentioned may not be directly related to the main task. Therefore, some of the questions that this text can answer may require additional information or context to provide a complete answer.', \"  The provided text appears to be instructions for using a software tool to perform a task involving data sources and destinations. The text mentions selecting a data source from a Microsoft Excel sheet and a destination database, specifically a SQL Server Native Client 11.0. It also mentions a Source Tables and Views window.\\n\\nSome questions that this text can answer include:\\n\\n* How do I select a data source for my Aspose.Words project?\\n* What are the steps to connect to a SQL Server Native Client 11.0 database using Aspose.Words?\\n* How do I view source tables and views in Aspose.Words?\\n* What are the different options available for selecting a destination window in Aspose.Words?\\n* Is there a limit to the number of sheets that can be selected from a single Excel file in Aspose.Words?\\n\\nIt's worth noting that the text mentions an evaluation copy of Aspose.Words, which suggests that the software may have a trial period or limited functionality in certain aspects.\", '  The provided text appears to be a set of instructions or guidance for using a software tool, specifically Aspose.Words, to perform certain tasks related to data analysis and reporting. The text mentions various regions and their corresponding data sources, as well as a tool called \"dbo.APAC_USAGE_HIST_SNAPSHOT_ALL_QTR\" that is used to retrieve data for certain regions. Additionally, the text mentions a \"Column Mappings Window\" that needs to be checked to ensure that all columns are mapped correctly.\\n\\nSome questions that this text can answer include:\\n\\n* What are the different regions and their corresponding data sources that can be used with Aspose.Words?\\n* How do you retrieve data for different regions using Aspose.Words?\\n* What is the purpose of the \"Column Mappings Window\" in Aspose.Words?\\n* How do you ensure that all columns are mapped correctly in Aspose.Words?\\n* What is the full version of Aspose.Words and how can you obtain it?', \"  The provided text appears to be a set of instructions or a guide for working with a database. It mentions various database tables and fields, as well as SQL queries that can be used to retrieve information from the database.\\n\\nSome of the questions that this text can answer include:\\n\\n* How to retrieve a list of users with a NULL Comex IT Flag and Country = 'Japan'\\n* How to retrieve user details in the format of FIGO_SALES.ALL_SALES_GENIE_USERS\\n* How to retrieve the username and user ID for a specific user\\n* How to fill in desired columns in a database table\\n\\nOverall, the text seems to be focused on working with a specific database and using SQL queries to retrieve and manipulate data.\", '  The provided text appears to be a set of instructions or a workflow for updating the Comex IT flag and loading data to a table called FIGO_SALES.ALL_SALES_GENIE_USERS. The text mentions various entities such as ANZ, SGMY, TH, KR, JP, and NPI, which suggests that it may be related to a business or organization with international operations.\\n\\nThe text also mentions several dashboards that need to be refreshed, including Sales Genie user analytics, NPI Genie user analytics, and user adoption dashboards for Japan/Korea and other regions.\\n\\nSome questions that this text can answer include:\\n\\n* What is the purpose of the APAC_Usage_Sales_Genie stored procedure?\\n* Which dashboards need to be refreshed, and how are they related to user adoption?\\n* What is the role of the Comex IT flag in this workflow?\\n* Which countries or regions are mentioned in the text, and how are they related to the workflow?\\n* What is the purpose of the FIGO_SALES.ALL_SALES_GENIE_USERS table, and how does it relate to the workflow?', \"  The provided text appears to be a portion of a document that discusses the use of Aspose.Words, a software tool for working with Microsoft Word documents. The text mentions that the document was created using an evaluation copy of Aspose.Words and encourages the reader to visit the company's website to learn more about the full version of the software.\\n\\nSome questions that this text can answer include:\\n\\n* What is Aspose.Words and what is it used for?\\n* What are some of the features and capabilities of Aspose.Words?\\n* How can I learn more about Aspose.Words and its full version?\\n* Is there a free evaluation copy of Aspose.Words available?\\n* What kind of documents can Aspose.Words handle and manipulate?\"]\n"
     ]
    }
   ],
   "source": [
    "def get_document_summaries(document_names):\n",
    "   \n",
    "    document_summaries = []  # Initialize an empty list to store document summaries\n",
    "    for doc_name in document_names:\n",
    "        summary = doc_summary_index.get_document_summary(doc_name)  # Get summary for each document\n",
    "        document_summaries.append(summary)  # Store the summary in the list\n",
    "    return document_summaries\n",
    "\n",
    "# Example usage:\n",
    "document_names =[\"split_by_page_1\",\"split_by_page_2\",\"split_by_page_3\",\"split_by_page_4\",\"split_by_page_5\",\"split_by_page_6\",\"split_by_page_7\",\"split_by_page_8\",\"split_by_page_9\"]\n",
    "summaries = get_document_summaries(document_names)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3cccd2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are the steps involved in refreshing a user adoption dashboard?', 'How do you take backups of dashboards and databases?', 'How do you download data from a dashboard into Excel format?', 'How do you add columns to an Excel sheet?', 'What is the purpose of refreshing a user adoption dashboard?', 'What kind of data is used to refresh a user adoption dashboard?', 'What are the different countries mentioned in the text?', 'What are the various functions and dashboards discussed in the text?', 'What is the purpose of the \"Name hist view\" column?', 'What is the difference between a cockpit and a non-cockpit dashboard?', 'What is the software tool Aspose.Words used for?', 'What is the purpose of the QARA Dashboard?', 'How do I filter data based on the local date and time in SQL?', 'What is the purpose of the Aspose.Words software, and what are its capabilities?', 'How do I download sales and RA dashboards for different regions?', 'What is the format of the data in the APAC_USAGE_HIST_SNAPSHOT_ALL_QTR table, and what information does it contain?', 'How do I use the convert() function in SQL to convert a date and time field to a datetime data type?', 'What is the difference between the All Business and RA dashboards, and how do they relate to the data in the APAC_USAGE_HIST_SNAPSHOT_ALL_QTR table?', 'What are the steps involved in preparing excel sheets for different regions?', 'How do you copy and paste data into a remote desktop using SSMS?', 'What is the procedure for importing data into a database using SSMS?', 'What software tools are required to carry out the task described in the text?', 'What is the purpose of the task described in the text?', 'How do I select a data source for my Aspose.Words project?', 'What are the steps to connect to a SQL Server Native Client 11.0 database using Aspose.Words?', 'How do I view source tables and views in Aspose.Words?', 'What are the different options available for selecting a destination window in Aspose.Words?', 'Is there a limit to the number of sheets that can be selected from a single Excel file in Aspose.Words?', 'What are the different regions and their corresponding data sources that can be used with Aspose.Words?', 'How do you retrieve data for different regions using Aspose.Words?', 'What is the purpose of the \"Column Mappings Window\" in Aspose.Words?', 'How do you ensure that all columns are mapped correctly in Aspose.Words?', 'What is the full version of Aspose.Words and how can you obtain it?', 'What is the purpose of the APAC_Usage_Sales_Genie stored procedure?', 'Which dashboards need to be refreshed, and how are they related to user adoption?', 'What is the role of the Comex IT flag in this workflow?', 'Which countries or regions are mentioned in the text, and how are they related to the workflow?', 'What is the purpose of the FIGO_SALES.ALL_SALES_GENIE_USERS table, and how does it relate to the workflow?', 'What is Aspose.Words and what is it used for?', 'What are some of the features and capabilities of Aspose.Words?', 'How can I learn more about Aspose.Words and its full version?', 'Is there a free evaluation copy of Aspose.Words available?', 'What kind of documents can Aspose.Words handle and manipulate?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_questions_from_list(text_list):\n",
    "    \"\"\"\n",
    "    Extract questions from a list of text strings.\n",
    "    \n",
    "    Args:\n",
    "    text_list (list of str): List of text strings.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of extracted questions.\n",
    "    \"\"\"\n",
    "    questions = []  # Initialize an empty list to store extracted questions\n",
    "    pattern = r'\\* (.+\\?)'  # Regular expression pattern to match questions\n",
    "    \n",
    "    for text in text_list:\n",
    "        extracted_questions = re.findall(pattern, text)  # Extract questions from each text string\n",
    "        questions.extend(extracted_questions)  # Extend the list of questions with the extracted ones\n",
    "    \n",
    "    return questions\n",
    "\n",
    "\n",
    "extracted_questions = extract_questions_from_list(summaries)\n",
    "print(extracted_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba9cdbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe provided text appears to be a set of instructions or guidelines for refreshing a dashboard related to user adoption. The text mentions various databases, tables, and queries that are used to take backups and download data in Excel format. Additionally, the text mentions three different regions: ANZ/SGMY/TH, KR/JP, and NPI.\\n\\nSome of the questions that this text can answer include:\\n\\n* What are the steps involved in refreshing the user adoption dashboard?\\n* What databases and tables are used to retrieve data for the dashboard?\\n* How is the data downloaded and manipulated to create the dashboard?\\n* What regions are included in the dashboard and how are their data handled differently?\\n* What is the purpose of the user adoption dashboard and what insights can it provide?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_summary_index.get_document_summary(\"split_by_page_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_summaries(document_names):\n",
    "   \n",
    "    document_summaries = []  # Initialize an empty list to store document summaries\n",
    "    for doc_name in document_names:\n",
    "        summary = doc_summary_index.get_document_summary(doc_name)  # Get summary for each document\n",
    "        document_summaries.append(summary)  # Store the summary in the list\n",
    "    return document_summaries\n",
    "\n",
    "# Example usage:\n",
    "wiki_titles=[\"split_by_page_1\",\"split_by_page_2\",\"split_by_page_3\",\"split_by_page_4\",\"split_by_page_5\"]\n",
    "summaries = get_document_summaries(document_names)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f05e0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"index\")\n",
    "doc_summary_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c6192de",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = doc_summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57dd16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage,MessageRole\n",
    "\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=(\n",
    "            \"Summarize the documents.\\n\"\n",
    "             \"Always answer the query using the provided context information,\"\n",
    "        ),\n",
    "    ), \n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information , \"\n",
    "    \"answer the question: {query_str} in bullet points or numbered list where appropriate.\\n\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n",
    "query=\"\"\"please summarize the documents\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8312c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d954cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here's a summary of the document in bullet points:\n",
      "\n",
      "• User adoption is used to check how many users have used the sales Genie Dashboard in the last one week.\n",
      "\n",
      "• To refresh the dashboard, take backups of all three dashboards from Tableau and update the data in excel for all six regions.\n",
      "\n",
      "• After downloading the data, open the excel sheet, delete the first column, and add two columns (Country & function) in each region.\n",
      "\n",
      "• Filter the data according to created_at_local in SQL.\n",
      "\n",
      "• Take a filtered data in excel and copy that data in a new excel sheet and save it.\n",
      "\n",
      "• Open a remote desktop connection, go to SSMS, copy all excels, and paste them in the Bang Account remote desktop.\n",
      "\n",
      "• Connect to the test environment, select the database (APACBI_FIGO_SALES), right-click, select Task, Import data, and choose a data source window.\n",
      "\n",
      "• Select Data source as Micro soft excel, browse your excel sheet (single sheet at a time), click Next, and choose a Destination window.\n",
      "\n",
      "• Select data base (SQL Server Native client 11.0), check for server name it should be Test Environment, click Next, and select destination as per region.\n",
      "\n",
      "• Select Source Tables and Views Window, select destination as per region, and click Ok.\n",
      "\n",
      "• Check the users with NULL Comex IT Flag and get the user details as per the FIGO_SALES.ALL_SALES_GENIE_USERS format.\n",
      "\n",
      "• Fill in below columns as desired and run below Stored Procedures.\n",
      "\n",
      "• Refresh below Dashboards and the user adoption dashboard is in QA server.\n",
      "\n",
      "I hope this summary helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30caf4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1=\"What are User Adoption Execution Steps? Summarize.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ac9a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2f64a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here are the User Adoption Execution Steps:\n",
      "\n",
      "1. Take backups for all 3 dashboards from Tableau.\n",
      "2. Connect to test database in SQL.\n",
      "3. Take backups for all tables from the database.\n",
      "4. Download data in excel for all 6 regions.\n",
      "5. Open the excel sheet and delete the first column.\n",
      "6. Add two columns (Country & function) in each region.\n",
      "7. Remove 1 or 2 next to the QARA Dashboard in the NPI column.\n",
      "8. Check for data created at local date and time.\n",
      "9. Filter the data according to created_at_local in SQL.\n",
      "10. Take a filtered data in excel and copy that data in a new excel sheet and save it.\n",
      "11. Open the remote desktop connection and go to SSMS.\n",
      "12. Copy all excels and paste in Bang Account remote desktop.\n",
      "13. Go to SSMS and connect to the test environment.\n",
      "14. Select Database (APACBI_FIGO_SALES) and right-click.\n",
      "15. Select Task and Import data.\n",
      "16. Choose a data source window and select Micro soft excel.\n",
      "17. Browse your excel sheet (single sheet at a time) and click Next.\n",
      "18. Choose a Destination window and select data base (SQL Server Native client 11.0).\n",
      "19. Check for server name it should be Test Environment and click Next.\n",
      "20. Select destination as per region.\n",
      "21. Select source tables and views window and select destination as per region.\n",
      "22. Check whether all the columns are mapped correctly and click Ok.\n",
      "23. The window will appear as shown below. This window will give the details of success rows.\n",
      "24. Check the users with NULL Comex IT Flag.\n",
      "25. Get the user details as per the FIGO_SALES.ALL_SALES_GENIE_USERS format.\n",
      "26. Fill in below columns as desired.\n",
      "27. Run below Stored Procedures.\n",
      "28. Refresh below Dashboards.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12c0ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2=\"Which link I have to use for taking backup of dashboard?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5102021",
   "metadata": {},
   "outputs": [],
   "source": [
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a1451aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "* Use the link <https://tableau-qa.medtronic.com/#/site/APACBI> to access the QA server.\n",
      "* Click on the \"Sales Genie\" tab.\n",
      "* Select the \"Sales Genie user analytics\" option for ANZ/SGMY/TH, \"Sales Genie user analytics(JP_KR)\" for KR/JP, and \"NPI Genie User Anlaytics\" for NPI.\n",
      "* Click on the \"Download\" button to download the data in excel for all 6 regions.\n",
      "* Use the link <https://tableau.medtronic.com/#/views/TableauUsageDetails_16226544435390/TraffictoViews?:iid=1> to download the data in excel for all 6 regions.\n",
      "\n",
      "Please note that the provided links are for reference purposes only and may not be accessible outside of the Medtronic network.\n"
     ]
    }
   ],
   "source": [
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f5991e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "query3=\" Provide me details of “MDT Network Ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f54d41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b077978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are the details of \"MDT Network Ids\" in bullet points:\n",
      "\n",
      "* MDT Network Ids are used to identify and manage user adoption in the sales Genie Dashboard.\n",
      "* The Ids are created and managed by the Medtronic IT team.\n",
      "* There are four MDT Network Ids for different regions: ANZ/SGMY/TH, KR/JP, NPI, and Japan/Korea.\n",
      "* Each Id has a unique name and description, such as \"ANZ/SGMY/TH - Sales Genie User Analytics\" and \"KR/JP - Sales Genie User Analytics (JP_KR)\".\n",
      "* The Ids are used to filter and analyze user adoption data in the sales Genie Dashboard.\n",
      "* The data is refreshed every Friday in QA, and the MDT Network Ids are used to identify and update the data for each region.\n",
      "* The updated data is then loaded into the FIGO_SALES.ALL_SALES_GENIE_USERS table.\n",
      "* The MDT Network Ids are also used to run stored procedures, such as dbo.APAC_Usage_Sales_Genie, to update the Comex IT flag for users.\n",
      "* The flag is updated based on the user's country and region.\n",
      "* The updated flag is then reflected in the sales Genie Dashboard for each region.\n",
      "* The MDT Network Ids are also used to refresh the dashboards for each region.\n",
      "* The refreshed dashboards provide up-to-date user adoption data for each region.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "57e62982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the steps involved in refreshing a user adoption dashboard:\n",
      "\n",
      "• Take backups for all three dashboards from Tableau.\n",
      "• Connect to the test database in SQL.\n",
      "• Take backups for all tables from the database.\n",
      "• For ANZ/SGMY/TH, use two queries to take a backup.\n",
      "• For KR/JP, use two queries to take a backup.\n",
      "• For NPI, use one query to take a backup.\n",
      "• Download the data in Excel for all six regions using a link.\n",
      "• Open the Excel sheet and delete the first column.\n",
      "• Add two columns (Country & Function) in each region before Column ‘K’.\n",
      "\n",
      "I hope that helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "query_1='What are the steps involved in refreshing a user adoption dashboard?'\n",
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83320bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the steps involved in refreshing a user adoption dashboard:\n",
      "\n",
      "• Take backups for all three dashboards from Tableau.\n",
      "• Connect to the test database in SQL.\n",
      "• Take backups for all tables from the database.\n",
      "• For ANZ/SGMY/TH, use two queries to take a backup.\n",
      "• For KR/JP, use two queries to take a backup.\n",
      "• For NPI, use one query to take a backup.\n",
      "• Download the data in excel for all six regions using the provided link.\n",
      "• Open the excel sheet and delete the first column.\n",
      "• Add two columns (Country & Function) in each region.\n",
      "• The first step in refreshing the user adoption dashboard is taking backups for all three dashboards from Tableau.\n",
      "• The second step is connecting to the test database in SQL.\n",
      "• The third step is taking backups for all tables from the database.\n",
      "• The fourth step is using two queries to take a backup for ANZ/SGMY/TH.\n",
      "• The fifth step is using two queries to take a backup for KR/JP.\n",
      "• The sixth step is using one query to take a backup for NPI.\n",
      "• The seventh step is downloading the data in excel for all six regions using the provided link.\n",
      "• The eighth step is opening the excel sheet and deleting the first column.\n",
      "• The ninth step is adding two columns (Country & Function) in each region.\n"
     ]
    }
   ],
   "source": [
    "query_2='How do you take backups of dashboards and databases?'\n",
    "response__2 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "337729e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the steps involved in refreshing a user adoption dashboard:\n",
      "\n",
      "• Take backups for all three dashboards from Tableau.\n",
      "• Connect to the test database in SQL.\n",
      "• Take backups for all tables from the database.\n",
      "• For ANZ/SGMY/TH, use two queries to take a backup.\n",
      "• For KR/JP, use two queries to take a backup.\n",
      "• For NPI, use one query to take a backup.\n",
      "• Download the data in Excel for all six regions using a link.\n",
      "• Open the Excel sheet and delete the first column.\n",
      "• Add two columns (Country & Function) in each region before Column ‘K’.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "query_2='How do you download data from a dashboard into Excel format?'\n",
    "response__2 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "47440ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the steps involved in refreshing a user adoption dashboard:\n",
      "\n",
      "• Take backups for all three dashboards from Tableau.\n",
      "• Connect to the test database in SQL.\n",
      "• Take backups for all tables from the database.\n",
      "• For ANZ/SGMY/TH, use two queries to take a backup.\n",
      "• For KR/JP, use two queries to take a backup.\n",
      "• For NPI, use one query to take a backup.\n",
      "• Download the data in excel for all six regions using a link.\n",
      "• Open the excel sheet and delete the first column.\n",
      "• Add two columns (Country & Function) in each region.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_2='How do you add columns to an Excel sheet?'\n",
    "response__2 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "834ff37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the steps involved in refreshing a user adoption dashboard:\n",
      "\n",
      "• Take backups for all three dashboards from Tableau.\n",
      "• Connect to the test database in SQL.\n",
      "• Take backups for all tables from the database.\n",
      "• For ANZ/SGMY/TH, use two queries to take a backup.\n",
      "• For KR/JP, use two queries to take a backup.\n",
      "• For NPI, use one query to take a backup.\n",
      "• Download the data in Excel for all six regions using a link.\n",
      "• Open the Excel sheet and delete the first column.\n",
      "• Add two columns (Country & Function) in each region before Column ‘K’.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "query_2='What is the purpose of refreshing a user adoption dashboard?'\n",
    "\n",
    "response__2 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f95ab575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the steps involved in refreshing a user adoption dashboard:\n",
      "\n",
      "• Take backups for all three dashboards from Tableau.\n",
      "• Connect to the test database in SQL.\n",
      "• Take backups for all tables from the database.\n",
      "• For ANZ/SGMY/TH, use two queries to take a backup.\n",
      "• For KR/JP, use two queries to take a backup.\n",
      "• For NPI, use one query to take a backup.\n",
      "• Download the data in Excel for all six regions using a link.\n",
      "• Open the Excel sheet and delete the first column.\n",
      "• Add two columns (Country & Function) in each region before Column ‘K’.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "query_2='What kind of data is used to refresh a user adoption dashboard?'\n",
    "\n",
    "response__2 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a0e29078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage,MessageRole\n",
    "\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=(\n",
    "            \"Summarize the documents.\\n\"\n",
    "             \"Always answer the query using the provided context information, \"\n",
    "            \n",
    "\n",
    "        ),\n",
    "    ), \n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information , \"\n",
    "    \"answer the question: {query_str} in bullet points or numbered list where appropriate.\\n\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cc740682",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"'What are the different countries mentioned in the text?', \n",
    "•\t'What are the various functions and dashboards discussed in the text?', \n",
    "•\t'What is the purpose of the \"Name hist view\" column?', \n",
    "•\t'What is the difference between a cockpit and a non-cockpit dashboard?',\n",
    "•\t'What is the software tool Aspose.Words used for?',\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a6b12955",
   "metadata": {},
   "outputs": [],
   "source": [
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "92b55fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, here are the answers to the questions:\n",
      "\n",
      "•\tWhat are the different countries mentioned in the text?\n",
      "\n",
      "\tANZ\n",
      "\tAPAC\n",
      "\n",
      "•\tWhat are the various functions and dashboards discussed in the text?\n",
      "\n",
      "\tSales Dashboard\n",
      "\tRA Dashboard\n",
      "\n",
      "•\tWhat is the purpose of the \"Name hist view\" column?\n",
      "\n",
      "\tTo check for data created at local date and time\n",
      "\n",
      "•\tWhat is the difference between a cockpit and a non-cockpit dashboard?\n",
      "\n",
      "\tNot mentioned in the text\n",
      "\n",
      "•\tWhat is the software tool Aspose.Words used for?\n",
      "\n",
      "\tCreating and editing Word documents\n"
     ]
    }
   ],
   "source": [
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "51fc11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"'What is the purpose of the QARA Dashboard?',\n",
    "'How do I filter data based on the local date and time in SQL?', \n",
    "'What is the purpose of the Aspose.Words software, and what are its capabilities?',\n",
    "'How do I download sales and RA dashboards for different regions?',\n",
    "'What is the format of the data in the APAC_USAGE_HIST_SNAPSHOT_ALL_QTR table, and what information does it contain?',\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "971fa050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, here are the answers to the questions:\n",
      "\n",
      "What is the purpose of the QARA Dashboard?\n",
      "\n",
      "* The QARA Dashboard is used to display and analyze data related to sales and RA (Returns, Allowances, and Adjustments) for different regions.\n",
      "\n",
      "How do I filter data based on the local date and time in SQL?\n",
      "\n",
      "* To filter data based on the local date and time in SQL, you can use the convert() function to convert the created_at_local column to a datetime data type, and then use the order by clause to sort the data in descending order based on the created_at_local column.\n",
      "\n",
      "What is the purpose of the Aspose.Words software, and what are its capabilities?\n",
      "\n",
      "* Aspose.Words is a software that allows you to create, modify, and convert Word documents programmatically. It can be used to generate documents, extract data from documents, and perform various other operations on Word documents.\n",
      "\n",
      "How do I download sales and RA dashboards for different regions?\n",
      "\n",
      "* To download sales and RA dashboards for different regions, you can use the download links provided in the document, such as \"Download Sales Dashboard\" and \"Download RA Dashboard\". The links are region-specific, so you need to click on the appropriate link for the region you want to download the dashboard for.\n",
      "\n",
      "What is the format of the data in the APAC_USAGE_HIST_SNAPSHOT_ALL_QTR table, and what information does it contain?\n",
      "\n",
      "* The APAC_USAGE_HIST_SNAPSHOT_ALL_QTR table contains historical usage data for the APAC region, broken down by quarter. The table contains columns such as Country, Region, Customer, Product, and Usage, and provides information on the usage of different products by customers in different countries and regions.\n"
     ]
    }
   ],
   "source": [
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query)\n",
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cc054e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  •\tTo use the convert() function in SQL to convert a date and time field to a datetime data type, you can use the following syntax: convert(datetime, created_at_local)\n",
      "\n",
      "•\tThe All Business dashboard and RA dashboard are both used to view data related to business performance, but the RA dashboard is more focused on providing insights into specific regions, whereas the All Business dashboard provides a broader overview of business performance across all regions. The data in the APAC_USAGE_HIST_SNAPSHOT_ALL_QTR table is related to the RA dashboard, as it contains data related to usage patterns in the APAC region.\n",
      "\n",
      "•\tPreparing excel sheets for different regions involves selecting the appropriate data for that region, filtering and sorting the data as needed, and then creating the desired visualizations and reports.\n",
      "\n",
      "•\tTo copy and paste data into a remote desktop using SSMS, you can use the \"Copy with headers\" option in the SSMS query editor to copy the data to the clipboard, and then use the \"Paste\" option in the remote desktop to paste the data into the desired location.\n",
      "\n",
      "•\tThe procedure for importing data into a database using SSMS involves creating a new query in the SSMS query editor, specifying the source data file or files, selecting the appropriate database and table to import the data into, and then executing the query to import the data.\n",
      "\n",
      "•\tThe software tools required to carry out the task described in the text include Aspose.Words, SQL Server Management Studio (SSMS), and Excel.\n",
      "\n",
      "•\tThe purpose of the task described in the text is to prepare and analyze data related to business performance in different regions, in order to gain insights and make informed decisions.\n",
      "\n",
      "•\tTo select a data source for your Aspose.Words project, you can use the \"Data Source\" option in the Aspose.Words ribbon to select a data source from a list of available data sources, or you can create a new data source by specifying the connection details and data source type.\n"
     ]
    }
   ],
   "source": [
    "query_1=\"\"\"'How do I use the convert() function in SQL to convert a date and time field to a datetime data type?'What is the difference between the All Business and RA dashboards, and how do they relate to the data in the APAC_USAGE_HIST_SNAPSHOT_ALL_QTR table?',\n",
    "•\t'What are the steps involved in preparing excel sheets for different regions?', \n",
    "•\t'How do you copy and paste data into a remote desktop using SSMS?',\n",
    "•\t'What is the procedure for importing data into a database using SSMS?', \n",
    "•\t'What software tools are required to carry out the task described in the text?',\n",
    "•\t'What is the purpose of the task described in the text?',\n",
    "•\t'How do I select a data source for my Aspose.Words project?',\"\"\"\n",
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "830d0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  •\tTo connect to a SQL Server Native Client 11.0 database using Aspose.Words, follow these steps:\n",
      "\n",
      "\t1.\tSelect Data source as Microsoft Excel.\n",
      "\t2.\tBrowse your excel sheet (single sheet at a time).\n",
      "\t3.\tClick Next.\n",
      "\t4.\tSelect data base (SQL Server Native client 11.0).\n",
      "\t5.\tCheck for server name it should be Test Environment.\n",
      "\t6.\tClick Next.\n",
      "\n",
      "•\tTo view source tables and views in Aspose.Words, follow these steps:\n",
      "\n",
      "\t1.\tClick Next.\n",
      "\t2.\tSource Tables and Views Window will appear.\n",
      "\n",
      "•\tThe different options available for selecting a destination window in Aspose.Words are:\n",
      "\n",
      "\t1.\tMicrosoft Excel.\n",
      "\n",
      "•\tThere is no limit to the number of sheets that can be selected from a single Excel file in Aspose.Words.\n",
      "\n",
      "•\tThe different regions and their corresponding data sources that can be used with Aspose.Words are:\n",
      "\n",
      "\t1.\tData source as Microsoft Excel.\n",
      "\n",
      "•\tTo retrieve data for different regions using Aspose.Words, follow these steps:\n",
      "\n",
      "\t1.\tSelect the desired region.\n",
      "\t2.\tClick Next.\n",
      "\t3.\tSelect data base (SQL Server Native client 11.0).\n",
      "\t4.\tCheck for server name it should be Test Environment.\n",
      "\t5.\tClick Next.\n",
      "\t6.\tSource Tables and Views Window will appear.\n",
      "\n",
      "It is important to note that the above steps are based on the provided context information and may not be the only way to perform the tasks mentioned.\n"
     ]
    }
   ],
   "source": [
    "query_1=\"\"\"'What are the steps to connect to a SQL Server Native Client 11.0 database using Aspose.Words?',\n",
    "•\t'How do I view source tables and views in Aspose.Words?',\n",
    "•\t'What are the different options available for selecting a destination window in Aspose.Words?', \n",
    "•\t'Is there a limit to the number of sheets that can be selected from a single Excel file in Aspose.Word?\n",
    "•\t'What are the different regions and their corresponding data sources that can be used with Aspose.Words?',\n",
    "•\t'How do you retrieve data for different regions using Aspose.Words?',\"\"\"\n",
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8474d0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the answers to the questions:\n",
      "\n",
      "•\tThe purpose of the \"Column Mappings Window\" in Aspose.Words is to allow users to check whether all columns are mapped correctly.\n",
      "\n",
      "•\tTo ensure that all columns are mapped correctly in Aspose.Words, users can click Ok after verifying that the columns are correctly mapped in the Column Mappings Window.\n",
      "\n",
      "•\tThe full version of Aspose.Words can be obtained by visiting the Aspose website and discovering the full versions of their APIs.\n",
      "\n",
      "•\tThe purpose of the APAC_Usage_Sales_Genie stored procedure is not specified in the given context information.\n",
      "\n",
      "•\tThe dashboards that need to be refreshed are not specified in the given context information. However, it is mentioned that the ANZ/TH/SGMY usage dashboard needs to be refreshed.\n",
      "\n",
      "•\tThe Comex IT flag is not specified in the given context information. Therefore, its role in this workflow is unknown.\n",
      "\n",
      "It's worth noting that the provided context information is limited to a single page of a document, and therefore, it may not contain all the necessary information to answer the questions fully.\n"
     ]
    }
   ],
   "source": [
    "query_1=\"\"\"'What is the purpose of the \"Column Mappings Window\" in Aspose.Words?', \n",
    "•\t'How do you ensure that all columns are mapped correctly in Aspose.Words?', \n",
    "•\t'What is the full version of Aspose.Words and how can you obtain it?',\n",
    "•\t'What is the purpose of the APAC_Usage_Sales_Genie stored procedure?', \n",
    "•\t'Which dashboards need to be refreshed, and how are they related to user adoption?',\n",
    "•\t'What is the role of the Comex IT flag in this workflow?',\"\"\"\n",
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a16d9f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the answers to the questions you asked:\n",
      "\n",
      "•\tJapan, Korea, ANZ, TH, SG, and MY are the countries or regions mentioned in the text. They are related to the workflow as they are the destinations that can be selected for usage.\n",
      "\n",
      "•\tThe purpose of the FIGO_SALES.ALL_SALES_GENIE_USERS table is not specified in the given text. However, based on the name of the table, it appears to be related to sales data or user information for a genie usage. It is mentioned in the context of column mappings, which suggests that it may be used in the workflow to map columns from one data source to another.\n",
      "\n",
      "•\tAspose.Words is a software tool that allows users to create, modify, and manipulate Microsoft Word documents programmatically. It is developed by Aspose Pty Ltd and has been available since 2003.\n",
      "\n",
      "•\tAspose.Words has a wide range of features and capabilities, including the ability to read and write Word documents in various formats, create and edit documents from scratch, add text, images, and other elements to documents, and perform advanced operations such as mail merging and document automation. It also supports various programming languages, including .NET, Java, Python, Ruby, and Node.js.\n",
      "\n",
      "•\tTo learn more about Aspose.Words and its full version, you can visit the Aspose website at https://products.aspose.com/words/. The website provides detailed documentation, tutorials, and examples of how to use the software, as well as information on pricing and licensing.\n",
      "\n",
      "•\tYes, there is a free evaluation copy of Aspose.Words available. You can download it from the Aspose website. The evaluation copy allows you to try out the software's features and capabilities before deciding whether to purchase a license.\n",
      "\n",
      "•\tAspose.Words can handle and manipulate various types of Word documents, including DOC, DOCX, DOT, DOTX, and ODT. It can also read and write documents in other formats, such as PDF, XPS, and EPUB. Additionally, Aspose.Words supports working with documents that contain various elements, such as text, images, tables, charts, and forms.\n"
     ]
    }
   ],
   "source": [
    "query_1=\"\"\"Which countries or regions are mentioned in the text, and how are they related to the workflow?\n",
    "•'What is the purpose of the FIGO_SALES.ALL_SALES_GENIE_USERS table, and how does it relate to the workflow?', \n",
    "•'What is Aspose.Words and what is it used for?', \n",
    "•'What are some of the features and capabilities of Aspose.Words?', \n",
    "•'How can I learn more about Aspose.Words and its full version?', \n",
    "•'Is there a free evaluation copy of Aspose.Words available?',\n",
    "•'What kind of documents can Aspose.Words handle and manipulate?'\"\"\"\n",
    "response__1 = doc_summary_index.as_query_engine(text_qa_template=text_qa_template).query(query_1)\n",
    "print(response__1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
